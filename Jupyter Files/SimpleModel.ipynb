{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b69d9621-3a15-4d46-bde8-cd058b206d23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from collections import defaultdict\n",
    "from typing import Tuple\n",
    "from Enviorment import CustomEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd9dafd7-9cf1-4b79-abb1-3d85704455eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class learningAgent:\n",
    "    def __init__(self,action_space,learning_rate=0.1,discount_factor=0.99,exploration_rate=1.0,exploration_decay=0.995):\n",
    "        self.action_space=action_space\n",
    "        self.lr=learning_rate\n",
    "        self.df=discount_factor\n",
    "        self.er=exploration_rate\n",
    "        self.ed=exploration_decay\n",
    "        self.q_table=defaultdict(lambda:np.zeros(action_space.n))\n",
    "        \n",
    "    def chose_action(self,state):\n",
    "        if random.uniform(0,1)<self.er:\n",
    "            return self.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "    def learn(self,state,action,reward,next_state,done):\n",
    "        best_next_action=np.argmax(self.q_table[next_state])\n",
    "        temporal_difference=reward+self.df*self.q_table[next_state][best_next_action]*(1-done)\n",
    "        td_delta=temporal_difference-self.q_table[state][action]\n",
    "        self.q_table[state][action]+=self.lr*td_delta\n",
    "        if done:\n",
    "            self.er*=self.ed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0514addb-b622-4725-8580-8a7d70f3ac52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m env\u001b[38;5;241m=\u001b[39mCustomEnv()\n\u001b[0;32m      2\u001b[0m agent\u001b[38;5;241m=\u001b[39mlearningAgent(env\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[0;32m      3\u001b[0m num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m\n",
      "File \u001b[1;32m~\\Desktop\\Reinforcemtn\\Enviorment.py:18\u001b[0m, in \u001b[0;36mCustomEnv.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28msuper\u001b[39m(CustomEnv,\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m=\u001b[39mspaces\u001b[38;5;241m.\u001b[39mdict({\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m:spaces\u001b[38;5;241m.\u001b[39mBox(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m95.8\u001b[39m,high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m105.0\u001b[39m,shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,),dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymptoms\u001b[39m\u001b[38;5;124m\"\u001b[39m: spaces\u001b[38;5;241m.\u001b[39mMultiBinary(\u001b[38;5;241m5\u001b[39m),\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage_group\u001b[39m\u001b[38;5;124m\"\u001b[39m: spaces\u001b[38;5;241m.\u001b[39mDiscrete(\u001b[38;5;241m4\u001b[39m) })\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m=\u001b[39mspaces\u001b[38;5;241m.\u001b[39mdict({\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtreatment\u001b[39m\u001b[38;5;124m\"\u001b[39m:spaces\u001b[38;5;241m.\u001b[39mDiscrete(\u001b[38;5;241m5\u001b[39m),\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdosage\u001b[39m\u001b[38;5;124m\"\u001b[39m:spaces\u001b[38;5;241m.\u001b[39mBox(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100.0\u001b[39m,shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,),dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)})\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m:np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m98.6\u001b[39m],dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymptoms\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m],dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m),\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage_group\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m2\u001b[39m}\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "env=CustomEnv()\n",
    "agent=learningAgent(env.action_space)\n",
    "num_episodes=1000\n",
    "for episodes in range (num_episodes):\n",
    "    state=env.reset()\n",
    "    state=tuple(map(tuple,state))\n",
    "    done=False\n",
    "    total_reward=0\n",
    "    while not done:\n",
    "        action=agent.action_space.sample()\n",
    "        next_state, reward, done, info=env.step(action)\n",
    "        next_state=tuple(next_state)\n",
    "        agent.learn(tuple(state),action,reward,next_state,done)\n",
    "        state=next_state\n",
    "        total_reward+=reward\n",
    "    print(f\"Episode {episodes+1}/{num_episodes}-Total Reward:{total_reward}\")\n",
    "env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4531417e-a2cb-482f-bc82-ba819dcd3af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class improved_lr:\n",
    "    def __init__(self,action_space,learning_rate=0.01,decaying_rate=0.99,exploration_rate=1.0,exploration_decay=0.995):\n",
    "        self.action_space=action_space\n",
    "        self.lr=learning_rate\n",
    "        self.dr=decaying_rate\n",
    "        self.er=exploration_rate\n",
    "        self.ed=exploration_decay\n",
    "        self.q_table=\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530c6ed2-a782-4a15-a116-37e2a97c30b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('CartPole-v1')\n",
    "agent=learningAgent(env.action_space)\n",
    "num_episodes=1000\n",
    "for episodes in range (num_episodes):\n",
    "    state=env.reset()\n",
    "    state=tuple(map(tuple,state))\n",
    "    done=False\n",
    "    total_reward=0\n",
    "    while not done:\n",
    "        action=agent.chose_action(state)\n",
    "        result=env.step(action)\n",
    "        next_state, reward, done, info=result[:4]\n",
    "        next_state=tuple(next_state)\n",
    "        agent.learn(tuple(state),action,reward,next_state,done)\n",
    "        state=next_state\n",
    "        total_reward+=reward\n",
    "    print(f\"Episode {episodes+1}/{num_episodes}-Total Reward:{total_reward}\")\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
